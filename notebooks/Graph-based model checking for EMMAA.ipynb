{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Based Model Checking for EMMAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of pathfinding over directed graphs as a means of checking EMMAA models against tests. Statements and tests from the Ras Machine 2.0 model are used to generate a directed graph and then the tests satisfied by paths in the directed graph are compared to those satisfied by the higher precision PySB/Kappa model assembly and analysis approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from indra.statements import *\n",
    "from emmaa.util import get_s3_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the latest model manager and test results JSON for the EMMAA Ras Machine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [2019-05-01 16:40:27] indra.preassembler.grounding_mapper - DEFT will not be available for grounding disambiguation.\n"
     ]
    }
   ],
   "source": [
    "s3 = get_s3_client()\n",
    "# Get the Model Manager containing the assembled INDRA Statements\n",
    "s3_obj = s3.get_object(Bucket='emmaa', Key='results/rasmachine/latest_model_manager.pkl')\n",
    "mm = pickle.loads(s3_obj['Body'].read())\n",
    "# Get the Test results JSON\n",
    "s3_obj = s3.get_object(Bucket='emmaa', Key='results/rasmachine/results_2019-04-30-17-51-24.json')\n",
    "res = json.load(s3_obj['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the assembled statements from the Model Manager and build a NetworkX DiGraph based on agent names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmts = mm.model.assembled_stmts\n",
    "len(stmts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "provenance = defaultdict(list)\n",
    "for stmt in stmts:\n",
    "    if not len(stmt.agent_list()) == 2:\n",
    "        continue\n",
    "    subj, obj = stmt.agent_list()\n",
    "    edges.append((subj.name, obj.name))\n",
    "    if isinstance(stmt, Complex):\n",
    "        edges.append((obj.name, subj.name))\n",
    "    provenance[(subj.name, obj.name)].append(stmt)\n",
    "g = nx.DiGraph()\n",
    "g.add_edges_from(edges)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tests and results from the result JSON and compile a list of (source, target) test pairs along with results obtained from the ModelChecker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tests = []\n",
    "for result in res[1:]:\n",
    "    code = result['result_json']['result_code']\n",
    "    if result['english_path']:\n",
    "        mc_path_len = len(result['english_path'][0])\n",
    "    else:\n",
    "        mc_path_len = 0\n",
    "    tj = result['test_json']\n",
    "    test_stmt = stmts_from_json([tj])[0]\n",
    "    if len(test_stmt.agent_list()) == 2:\n",
    "        subj, obj = test_stmt.agent_list()\n",
    "        if subj.name != obj.name:\n",
    "            g_tests.append((tj['type'], subj.name, obj.name, code, mc_path_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the directed graph compiled from the model statements we look for shortest paths connecting the pairs of test nodes and build a Pandas DataFrame with the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for stmt_type, subj, obj, code, mc_path_len in g_tests:\n",
    "    sp_text = ''\n",
    "    g_path_len = 0\n",
    "    if subj not in g:\n",
    "        g_code = 'SUBJECT_NODE_NOT_FOUND'\n",
    "    elif obj not in g:\n",
    "        g_code = 'OBJECT_NODE_NOT_FOUND'\n",
    "    else:\n",
    "        g_code = 'PATH_FOUND'\n",
    "        try:\n",
    "            sp = nx.shortest_path(g, subj, obj)\n",
    "            g_path_len = len(sp) - 1\n",
    "            sp_text = ' -> '.join(sp)\n",
    "        except nx.NetworkXNoPath:\n",
    "            g_code = 'NO_PATH'\n",
    "    rows.append((stmt_type, subj, obj, code, mc_path_len, g_code, g_path_len, sp_text))\n",
    "df = pd.DataFrame.from_records(rows, columns=['test_stmt_type', 'subj', 'obj', 'mc_code',\n",
    "                                              'mc_path_len', 'g_code', 'g_path_len', 'shortest_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some statistics from the results.\n",
    "First, the number of tests passed by the Model Checker versus the graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_path = df[(df.mc_code == 'PATHS_FOUND') | (df.mc_code == 'MAX_PATH_LENGTH_EXCEEDED')]\n",
    "g_path = df[df.g_code == 'PATH_FOUND']\n",
    "print(\"Number of MC tests passed: %d (%.1f%%)\" % (len(mc_path), 100*len(mc_path)/len(df)))\n",
    "print(\"Number of Graph tests passed: %d (%.1f%%)\" % (len(g_path), 100*len(g_path)/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the cases where both the ModelChecker and the Graph found a path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_path = mc_path[mc_path.g_code == 'PATH_FOUND']\n",
    "print(\"Number of tests passed by both MC and G: %d\" % len(both_path))\n",
    "both_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it ever happen that the ModelChecker finds a path where there is none in the graph? Apparently not, which is reassuring because the paths found by the ModelChecker should be a strict subset of those in the directed graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_path[mc_path.g_code != 'PATH_FOUND']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional sanity check, we look for cases where the ModelChecker yields a *shorter* path than the shortest path in the graph, which should not happen (and doesn't). The rows in the table below consist only of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.mc_code == 'PATHS_FOUND') & (df.mc_path_len < df.g_path_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, how often is it that tests failed by the ModelChecker are passed in the Graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_only_path = df[(df.mc_code != 'PATHS_FOUND') & (df.g_code == 'PATH_FOUND')]\n",
    "mc_fail = len(df[df.mc_code != 'PATHS_FOUND'])\n",
    "g_only = len(g_only_path)\n",
    "ratio = g_only / mc_fail\n",
    "print(\"Number of MC tests failed: %d\" % mc_fail)\n",
    "print(\"Number of tests failed by MC and passed by G: %d\" % g_only)\n",
    "print(\"Ratio: %.3f\" % ratio)\n",
    "g_only_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So approximately half of the tests that fail in the ModelChecker pass in the directed graph. Inspection of the specific cases highlights a few recurring scenarios.\n",
    "\n",
    "First, **statements involving protein families.** In some cases tests involve protein families rather than specific genes, e.g. \"BRAF activates ERK\", rather than the specific genes MAPK1 or MAPK3. One possibility is that the detailed PySB model doesn't contain sufficient detailed mechanistic statements involving families to find  a connection. However, the directed graph contains a number of (sometimes indirect) edges that create paths satisfying the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_only_path[g_only_path.obj == 'ERK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, **statements types not handled by the Model Checker.** Complex statements, which describe binding between two proteins, are currently not handled by the INDRA Model Checker for technical reasons. These were often trivially satisfied by links in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_only_path[g_only_path.mc_code == 'STATEMENT_TYPE_NOT_HANDLED']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, cases where the **specific state of the subject or object was not found in the model**. These represent cases where the graph is explicitly ignoring details of the subject or object state in determining the existence of a causal link. Determining the proportion of paths produced by the directed graph that are mechanistically defensible will require inspection of the provenance of the underlying statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_only_path[(g_only_path.mc_code == 'SUBJECT_MONOMERS_NOT_FOUND') |\n",
    "            (g_only_path.mc_code == 'OBSERVABLES_NOT_FOUND')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
